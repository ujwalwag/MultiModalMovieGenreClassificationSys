import os
from flask import Flask, request, jsonify, render_template
import torch
import torch.nn as nn
import numpy as np
import pickle
from PIL import Image
from torchvision import models, transforms

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
DEVICE = torch.device("cpu")
print(f"âœ… Using device: {DEVICE}")

app = Flask(__name__, static_folder='static', template_folder='templates')

GENRE_COLUMNS = [
    'Drama', 'Comedy', 'Romance', 'Thriller', 'Action',
    'Horror', 'Documentary', 'Animation', 'Music', 'Crime'
]

IMG_TF = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PyTorch Tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PyTorchTokenizer:
    def __init__(self, num_words=20000, oov_token="<OOV>"):
        self.num_words = num_words
        self.oov_token = oov_token
        self.word_index = {}
        self.index_word = {}
        self.word_counts = {}
        
    def fit_on_texts(self, texts):
        """Build vocabulary from texts"""
        word_counts = {}
        for text in texts:
            for word in text.split():
                word_counts[word] = word_counts.get(word, 0) + 1
        
        # Sort by frequency and limit to num_words
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        sorted_words = sorted_words[:self.num_words-1]  # -1 for OOV token
        
        # Build word_index and index_word
        self.word_index[self.oov_token] = 0
        for i, (word, _) in enumerate(sorted_words, 1):
            self.word_index[word] = i
            self.index_word[i] = word
            
    def texts_to_sequences(self, texts):
        """Convert texts to sequences of indices"""
        sequences = []
        for text in texts:
            sequence = []
            for word in text.split():
                if word in self.word_index:
                    sequence.append(self.word_index[word])
                else:
                    sequence.append(self.word_index[self.oov_token])
            sequences.append(sequence)
        return sequences

def pad_sequences(sequences, maxlen=200, padding='post', truncating='post'):
    """Pad sequences to fixed length"""
    padded_sequences = []
    for sequence in sequences:
        if len(sequence) > maxlen:
            if truncating == 'post':
                sequence = sequence[:maxlen]
            else:
                sequence = sequence[-maxlen:]
        else:
            if padding == 'post':
                sequence = sequence + [0] * (maxlen - len(sequence))
            else:
                sequence = [0] * (maxlen - len(sequence)) + sequence
        padded_sequences.append(sequence)
    return np.array(padded_sequences)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Text Model Definition â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GenreLSTM(nn.Module):
    def __init__(self, emb, hid=128, drop=0.3):
        super().__init__()
        vocab_size, emb_dim = emb.shape
        self.embedding = nn.Embedding(vocab_size, emb_dim)
        self.embedding.weight = nn.Parameter(torch.tensor(emb, dtype=torch.float32), requires_grad=False)
        self.lstm = nn.LSTM(emb_dim, hid, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(drop)
        self.fc = nn.Linear(hid * 2, len(GENRE_COLUMNS))

    def forward(self, x):
        lstm_out, _ = self.lstm(self.embedding(x))
        pooled = lstm_out.mean(dim=1)
        dropped = self.dropout(pooled)
        return self.fc(dropped)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/")
def home():
    return render_template("index.html")

@app.route("/predict", methods=["POST"])
def predict_text():
    data = request.get_json(silent=True) or {}
    plot = (data.get("plot") or data.get("text") or "").strip()
    if not plot:
        return jsonify({"error": "No plot provided"}), 400

    try:
        print("ğŸ” Loading tokenizer...")
        with open('models/tokenizer.pickle', 'rb') as f:
            tokenizer = pickle.load(f)

        print("ğŸ” Loading embedding matrix and model state...")
        embedding_matrix = np.load('models/embedding_matrix.npy')
        txt_state = torch.load('models/genre_classifier.pth', map_location=DEVICE)

        print("ğŸ” Initializing model...")
        model = GenreLSTM(embedding_matrix).to(DEVICE)
        model.load_state_dict(txt_state)
        model.eval()

        print("ğŸ” Tokenizing input...")
        seq = tokenizer.texts_to_sequences([plot])[0]
        arr = pad_sequences([seq], maxlen=200, padding='post', truncating='post')
        tensor = torch.tensor(arr, dtype=torch.long, device=DEVICE)

        print("ğŸ” Predicting...")
        with torch.no_grad():
            probs = torch.sigmoid(model(tensor))[0].cpu().numpy()

        top3 = probs.argsort()[-3:][::-1]
        print("âœ… Prediction done.")
        return jsonify({"genres": [GENRE_COLUMNS[i] for i in top3]})

    except Exception as e:
        print(f"âŒ Exception: {str(e)}")
        return jsonify({"error": f"Failed to predict: {str(e)}"}), 500


@app.route("/predict_image", methods=["POST"])
def predict_image():
    if 'poster' not in request.files:
        return jsonify({"error": "No poster uploaded"}), 400

    try:
        img = Image.open(request.files['poster'].stream).convert("RGB")
    except Exception:
        return jsonify({"error": "Invalid image file"}), 400

    try:
        model = models.resnet18(pretrained=False)
        model.fc = nn.Linear(model.fc.in_features, len(GENRE_COLUMNS))
        state = torch.load('models/poster_genre_classifier.pth', map_location=DEVICE)
        model.load_state_dict(state, strict=False)
        model.to(DEVICE).eval()

        tensor = IMG_TF(img).unsqueeze(0).to(DEVICE)
        with torch.no_grad():
            probs = torch.sigmoid(model(tensor))[0].cpu().numpy()
        top3 = probs.argsort()[-3:][::-1]
        return jsonify({"genres": [GENRE_COLUMNS[i] for i in top3]})

    except Exception as e:
        return jsonify({"error": f"Failed to predict image genre: {str(e)}"}), 500

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Launch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)

